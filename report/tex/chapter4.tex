\makeatletter                                                   
\def\input@path{{../}}                                          
\makeatother                                                    
\documentclass[../main.tex]{subfiles}                           
\begin{document}                                                
\chapter{A 4th party logistics optimizer}
\label{ch:appr}
This chapter explains the setup of our model, the fourth party logistics optimizer (4PLO).
The first section \cref{sec:alns}, desribes the metaheuristic approach and explains the overall algorithm of our optimization model. 
The following \crefrange{sec:represent}{sec:wild} go more into detail on the different parts of our 4PLO model.\par
The first of these \cref{sec:represent} explain how we have represented a solution in our model and what a vehicle schedule is.
The next \cref{sec:init} explain how we have chosen the initial solution.
Then \cref{sec:heur} present each of the heuristics included in the basic version of our model.
The next \cref{sec:choosing}, explains how our model is choosing a heuristic in a given iteration.
After which \cref{sec:weight} show how we made our model adapt the choice of heuristic throughout the iterations using historical data.
Then \cref{sec:accept} explain how we accept solutions based on simmulated annealing.
Finally \cref{sec:wild} present the wild escape algorithm used by our model to diversify away from a local optimum.

\section{The 4th party logistic optimizer}
\label{sec:alns}

A brief pseudocode of our implementation of the 4th party logistic optimizer is described in \cref{alg:alns}.  

\begin{algorithm}
    \caption{4th party logistic optimizer}\label{alg:alns}
    \begin{algorithmic}[1]
        \Function{alns}{heuristics H, wild heuristics W,}
        \State solution $s = generateInitialSolution()$
        \State solution $s_{best} = s$
        \State iterations since best solution found $i=0$
        \Repeat
            \If {$i>escape\ condition$} \label{escpCond}
                \State $s = wildEscape(s, s_{best}, W)$ 
            \EndIf
            \State $s' = s$
            \State select a heuristic, $h\in H$ based on selection parameters
            \State $s' = applyHeuristic(h,s')$
            \If {$f(s') < f(s_{best})$}
                \State $s_{best}=s'$
            \EndIf
            \If {$accept(s', s)$}
                \State $s = s'$
            \EndIf
            \State update selection parameters and escape condition
        \Until {stop condition met}
        \State
        \Return $s_{best}$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

The algorithm starts by picking an initial solution $s$, and then moves into a loop where it picks a heuristic $h$ and applies it to to the current solution, in each iteration.
It then updates the best solution and decides to accept the newly created solution or not until the stop condition is met.
At the start of the loop it checks if an escape condition is met where it will run the algorithm from \cref{alg:wild} on our current solution $s$. \par
In the following sections we will go more into detail on each part of the model.

\section{Solution representation}
\label{sec:represent}
In order for our model to solve the 4PL problem, choosing a good \textit{solution representation} is an important part of the algorithm. 
There  are many ways to represent a solution like matrix, binary matrix and permutation. 
It is tempting to represent our solution in a matrix, where each row represent a vehichles schedule. 
However we will be using a permutation as solution representation in this paper, where each \textit{vehicles schedule} $S_v$ will be separated by a $0$.  

\begin{figure}                                                                                     
    \centering                                                                                 
    \includegraphics[width=.65\textwidth]{solution}                                     
    \caption{Example of a solution representation of a problem with 5 orders and 3 vehicles. A vehicle schedule, represented by $S_v$, contain information for each vehicle $v$, on which and in what order a vehicle schould pick up and deliver orders. The dark green tiles represent a pickup, the red tiles represent a delivery. The $0$ tiles separate each vehicle schedule. The complete permutaion represent our solution representation $s$.}
    \label{fig:solution}
\end{figure}

\Cref{fig:solution} show an example of a solution $s$, with 5 orders and 3 vehicles. Each vehicle schedule indicate in which order a pickup and delivery should follow. 
The first time an order appears has to be the pickup (highlighted as dark green in the figure). 
In the same way the second time the same order number appears, it has to be the delivery. 
As an example, $S_3$ is first picking up the order 4 and then delivering the order right after, as delivering an order before a pickup is not possible. The same goes for $S_1$, which first picks up order 5, then picks up order 3 and then delivers each of them after. $S_2$ represent an empty vehicle schedule. \par 
The final part of the solution representation, $S_{dummy}$, shows a vehicle schedule that represent the orders that have not been assigned to a vehicle yet.
In this chapter we will continue to refer to $s$ as a \textit{solution} and $S_v$ as a \textit{vehicle schedule} represented as above.


\section{Initial Solution}
\label{sec:init}
We mentioned premature solutions in \cref{ch:methods}, that can lead to a problem of diversifying solutions in a pickup and delivery model. 
To avoid this problem we have chosen to start with an initial solution $s$ where no orders are assigned to any vehicle, ie. all orders are assigned to the dummy vehicle in the solution representation.  
Not only does this contribute to avoid the problem of premature solutions, it is also efficient in terms of running time. On top of that we design our model to be able to adapt to the problem on its own, regardless of the initial solution. 

\section{Heuristics}
\label{sec:heur}
This section presents the heuristics used by our algorithm.
The first three heuristics \crefrange{sec:swap}{sec:2opt}, we call \textit{shuffeling heuristics}. They try to quickly schuffle a given solution around to do intensification search. \par
Removal and reinsertion heuristics are well reserached tools used for both intensification and diversification when pickup and delivery problems, as we showed in \cref{ch:methods}. 
The last four heuristics, \crefrange{sec:rand}{sec:shaw}, we refer to here as \textit{removal and reinsertion heuristics}. Each of them contain one heuristic for removal and one heuristic for reinsertion.
These are used partly for diversification, and partly for intensification depending on the amount of solution elements, $q$, being reinserted.  

\subsection{Swap}
\label{sec:swap}
This heuristic simply tries to swap the pickup and delivery of two different orders until the first time it finds a fit. 
\Cref{fig:swap} illustrates a successful swap between two orders in a vehicle schedule. 

\begin{figure}                                                                    
    \centering                                                                                     
    \includegraphics[width=\textwidth]{swap_ill}                                     
    \caption{A swap heuristic performed on a vehicle schedule with three orders. The numbered nodes indicate an order location and the letters P and D indicate pickup and delivery respectively. Emphazised numbers in the vehicle schedule are selected by the heuristic.}
    \label{fig:swap}
\end{figure}

This heuristic is very efficient in terms of running time and jumps randomly around the solution space. It is only working on two orders at a time so it is an intensification heuristic.

\subsection{3-exchange}
\label{sec:exch}
The 3-exchange heuristic selects a random vehicle with at least two assigned orders, and performs an exchange of 3 assigned positions until a feasible new combination is found. A 3-exchange of position 2, 4 and 6 in a vehicle schedule is illustrated by \cref{fig:exch}. \newline

\begin{figure}                                                                                     
    \centering                                                                                     
    \includegraphics[width=\textwidth]{3-exchange_ill}                                     
    \caption{A 3-exchange heuristic performed on a vehicle schedule with three orders. The numbered nodes indicate an order location and the letters P and D indicate pickup and delivery respectively. Emphazised numbers in the vehicle schedule are selected by the heuristic.}
    \label{fig:exch}                                                                            
\end{figure}

This heuristic is very fast, as the exchanges are fast operations and checking if a vehicles schedule is feasible is also a very effective operation. Like the swap heuristic from the previous section this heuristic jumps randomly around the solution space and is an intensification heuristic.

\subsection{2-opt}
\label{sec:2opt}
The 2-opt heuristic is used in metaheuristics to solve vehicle routing problems. \cite{lin65} implemented the 2-opt heuristic for travelling salesmen problem and \cite{bullnheimer99} combined 2-opt with the ant system metaheuristic.
The 2-opt heuristic used in this paper is similar to the heuristic from \cite{carrabs07}.
\Cref{fig:2opt} illustrates one iteration of our 2-opt heuristic performed on a vehicle schedule.

\begin{figure}                                                                                     
    \centering                                                                                 
    \includegraphics[width=\textwidth]{2-opt_ill}                                     
    \caption{One 2-opt heuristic operation performed on a vehicle schedule with three orders. The numbers indicate an order and the letters P and D indicate pickup and delivery respectively. Emphazised numbers indicate the $S_{(i+1)j}$ selected vehicle schedule part to reverse.}
    \label{fig:2opt}
\end{figure}

\begin{algorithm}
    \caption{2-opt heuristic}\label{alg:2opt}
    \begin{algorithmic}[1]
        \Function{2-opt}{}
        \State select random vehicle $v$ with $|v|>2$
        \State vehicle schedule $S_v$
        \Repeat
        \State $S_{best} = S_v$
        \State $n = |S_v|$
        \For{$i<n$}
            \State $j=i+1$
            \For{$j<n$}
                \State current vehicle schedule $S_v' = S_{0i} + reverse(S_{(i+1)j}) + S_{(j+1)n}$ 
                \If {$f(S_v')<f(S_{best})$} 
                \State $S_{best} = S_v'$
                \EndIf
            \EndFor
        \EndFor
        \State $S_v = S_{best}$
        \Until {no further improvement found}
        \State 
        \Return $S_v$
        \EndFunction
    \end{algorithmic}
\end{algorithm}


The heuristic is described in \cref{alg:2opt} and it starts by selecting a random vehicle with more than 2 orders.
For the selected vehicle it divides up the vehicle schedule $S_v$ in 3 parts, $S_0i$, $S_{ij}$ and $S_{jn}$, where $n=|S_v|$. It does this for all possible combinations of $0<i<n$ and $i<j\leq n$.
All orders up until the index $i$, aswell as orders from the index $j+1$ until the end of the vehicle schedule are left unchanged.
Then the orders from the index $i+1$ until index $j$ are inserted in reverse order.
If the new vehicle schedule $S_v'$ has a smaller cost than the best vehicle schedule $S_{best}$, it is remembered as the new best vehicle schedule. 
After all iterations the best vehicle schedule $S_{best}$ is selected as the new $S_v$.
This operation is continued until no improvement can be made for $S_v$ ie. the best possible schedule for the selected vehicle has been found.

\subsection{Random fit}
\label{sec:rand}
In most problems there are solutions that you simply cannot find using pure logic. 
Constantly minimizing costs and searching for the most efficient solution possible could lead to a problem of diversification, and not really exploring the whole solution space. 
We therefore decided to make one remove and reinsert heuristic that does not have any specific priorities but rather moves around the solution space randomly. 

\begin{algorithm}
    \caption{Remove random and reinsert first fit heuristic}\label{alg:rand}
    \begin{algorithmic}[1]
        \Function{randomFit}{$s\in \{solution}$}
        \State select the number of orders to reinsert $q$
        \State solution $s' = s$
        \State remove $q$ orders from $s'$
        \State set I = removed orders
        \For{$z\in I$}
            \Repeat
            \State choose random vehicle $v$
            \State choose random position in vehicle schedule $S_v$
            \State insert $z$ in $S_v$
            \Until {feasible vehicle schedule found}
            \State update s'
        \EndFor
        \State
        \Return s'
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\Cref{alg:rand} shows the pseudocode for our remove random insert first fit heuristic. 
It starts by selecting a random amount of orders $q$ which it is to remove and reinsert. 
Then it selects $q$ random orders and removes them from the solution. 
We have chosen $q$ between 2 orders and $10\%$ of the amount of orders in the instance.
The heuristic then selects a random vehicle $v$ and inserts the removed orders in its vehicle schedule $S_v$ randomly until a feasible vehicle schedule is found.
It then updates the solution $s$ with the $S_v$.
This heuristic is used for diversifying the solution and is trying to search for possible solutions regardless of the cost they produce.

\subsection{Clustering}
\label{sec:clust}
\Cref{sec:4flow} illustrates how a typical 4PL customer typically have their factories and suppliers structured in clusters around a continent or a country. 
This led us to try to build a heuristic which considers clusters as a factor when removing and reinserting an order in a vehicle schedule. 
Orders delivered from and too different clusters by the same vehicle, should be removed. 
In the same way orders that are being delivered from and to similar clusters should be bundled together on vehicles. \par
To build a heuristic which takes advantage of this there challenges that needs to be overcome. 
How do we determine the size of our cluster? 
And after that how do we decide which locations belong in which cluster? 
The next two paragraphs will explain the methods we have used to solve this, before we go into detail on how our clustering heuristic is working.

\subsubsection{The siluette coefficient}
To decide which pickup and delivery locations belong to which clusters we first needed to find how many clusters we should have. 
In other words we need to find $k$, the amount of clusters best fitting to the problem. 
\cite{kaufman90} introduced an efficient way to compare clusters of different sizes by calculating the what they call a siluette coefficient.
The coefficient is calculated based on two aspects, cohesion $a_x$ and separation $b_x$ of a node (or in our case a location) $x$. 
Together these aspects make out the siluette $u$.
\begin{figure}
\centering
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cohesion}
        \caption{Cohesion}
        \label{fig:cohesion}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.60\textwidth}
        \centering
        \includegraphics[width=\textwidth]{separation}
        \caption{Separation}
        \label{fig:separation}
    \end{subfigure}
    \label{fig:silu}
    \caption{The figures show the cohesion and separation effect illustrated respectively by the red and yellow lines. The green node represent location $i$ and $Cl_i$ is its cluster. The cluster $Cl_h$ is the cluster with the minimum average distance to $i$. }
\end{figure}


\Cref{fig:silu} illustrate the cohesion and sparation of a green node $i$ part of cluster $C^L_i$. 
The cohesion $a_i$ is the mean of the distances, $d_{ij}$, from $i$ to all other nodes $j\in C^L_i$. The separation $b_i$ is the minimum of the mean of the distances $d_{ij}$ from node $i$ to the nodes $j\not\in C^L_h$ in all other clusters $h\in \Gamma$.
The two effects can be written as follows:
\begin{equation}
    \label{eq:cohesion}
    a_i = \dfrac{1}{|C^L_i|-1}\sum_{j\in C^L_i, i\neq j}{d_{ij}}
\end{equation}

\begin{equation}
    \label{eq:cohesion}
    b_i = \min_{h\neq i} \dfrac{1}{|C^L_h|}\sum_{i\neq j}{d_{ij}}
\end{equation}

The siluette coefficient $u_i$ for location $i$ is then calculated as follows: 

\begin{alignat} {2}
    \label{eq:siluette}
    a_i = \dfrac{b_i-a_i}{\max{a_i,b_i}}, &\quad\quad\quad\quad&& if\ |Cl_i|>1 \label{eq:siluette}\\
    a_i = 0,& && if\ |Cl_i| = 1 \nonumber
\end{alignat}

Comparing the siluette coefficient of one cluster to another requires that the nodes are divided into clusters. We will now explain how we found the best cluster for a given size $k$.

\subsubsection{Hirarchial single linkage clustering}
To find the best possible cluster of a given size $k$, we used something called hirarchial single linkage clustering algorithm.

\begin{figure}                                                                                     
    \centering                                                                                 
    \includegraphics[width=\textwidth]{single_linkage}                                     
    \caption{Each table represents a distance matrix between pairs of nodes. In the first table, $d_{bd}$ marked in yellow is the shortest distance and therefore chosen to be part of the same cluster. The new node, or cluster, $(b,d)$ contain the shortest distances to the other nodes. In the second table, $d_{ac}$ is shortest and therefore put together in a cluster.}
    \label{fig:hierarch}
\end{figure}

\cref{fig:hierarch} illustrates how the algorithm works. 
In each iteration we choose the pair of nodes, or locations, with the smallest distance, $d_{ij}$ to be a part of a new cluster. The new distance matrix contain the shortest distance from the merged nodes to any other node. The merging of nodes goes on until we reach $k$ amount of nodes.

\subsubsection{Removing and inserting elements based on clusters}
In the previous sections we described how we can compare clusters of different sizes $k$, and how to find the best cluster for a given size. We used this in our model by running a preprocessiong algorithm that found the best clusters for different sizes of $k$. And then compared those clusters using the siluette coefficient to find the best possible cluster for a given distance matrix. \par

\begin{algorithm}
    \caption{Cluster heuristic}\label{alg:cluster}
    \begin{algorithmic}[1]
        \Function{cluster}{$s\in \{solution}$, $p$}
        \State select the number of orders to reinsert $q$
        \State add orders to array A acending based on cluster value $c_i$
        \State empty order set $I$
        \Repeat
            \State choose a random number $y$ between $[0,1]$
            \State remove the order in position $y^p$ in $A$ from $s$ 
            \State add removed order to $I$
        \Until {$|I| = q$}
        \For{$i\in I$}
            \State find vehicle schedule $S_v$ which maximizes $c_i$
            \State insert $i$ in best possible position in $S_v$
            \State update $s$ based on $S_v$
        \EndFor
        \State
        \Return $s$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\Cref{alg:cluster} start by selecting the amount of orders to remove. Then it sorts all orders from the given solution $s$ according to the cluster value $c^L_i$. $c^L_i$ is calculated based on the locations visited by the vehicle, ie. which other orders are bundled together on a vehicle schedule.
We can define $c^L_i$ as:
\begin{equation}
    c^L_i = \dfrac{|L^P_{i}|+|L^D_{i}|}{|S_v|-2}
\end{equation}

Here the set $L^P_{i}$ contain the locations from the same cluster $C^{LP}_i$ as the pickup location $P$ of $i$ on the same vehicle schedule. 
$L^D_{i}$ is a set of locations from the same cluster $C^{LD}_i$ as the delivery location $D$ of $i$, visited in the same vehicle schedule.
The set $S_v$ is the vehicle schedule for vehicle $v$, ie. $|S_v|$ is the amount of locations visited. \par

To help explain how the $c^L_i$ works we give an example:
\begin{itemize}
    \item A vehicle schedule $S_1 =$ (1 2 1 2 3 3) 
    \item Order 1 has a pickup location in cluster A and delivery location in cluster B.
    \item Order 2 has a pickup location in cluster B and delivery location in cluster C.
    \item Order 3 has a pickup location in cluster C and delivery location in cluster B.
\end{itemize}

For each location, related to other orders, visited in cluster A, order 1 will be rewarded a point since its pickup is in cluster A. 
For each location visited in cluster B, related to other orders, order 1 will be rewarded a point.
The pickup of order 1 is in cluster A which is unvisited by other orders, so $|L^P_{1}| = 0$.
Therefore  $c^L_1$ is not increased.
The delivery of order 1 is in cluster B, which is visited during pickup of order 2 and delivery of order 3 and therefore $|L^D_{1}|=2$ which increases $c^L_i$. \par
With the same logic, for order 2 the $|L^P_{2}|=2$ and $|L^D_{2}| = 1$.
Order 3 gets $|L^P_{3}=1$ and $|L^D_3| = 2$.
To be able to compare these points with orders on other vehicles we divide the points on the maximum possible points possible which is the same as $|S_v|-2$, or in the example above $6-2 = 4$ locations. 
That leads us to $c^L_1 = \dfrac{2}{4}$ and $c^L_2 = c^L_3 = \dfrac{3}{4}$ in our example. \par

In continuation of explanation of \cref{alg:cluster} it uses the cluster ranking $c^L_i$ first to decide which orders to remove.
It will remove the orders with the lowest ranking, using some randomization, based on the value $p$. 
We choose to remove the order on the position $y^p$ in the $c_i$ sorted list of orders, where $y$ is a random number between $[0,1]$. We have chosen $p=4$ for all our heursitics.  \newline \par
To reinsert the orders we have chosen to try and insert the order by maximizing $c_i$. This means we find the vehicle schedule where the $c_i$ rank is the highest, and insert the order in the best possible position in the chosen vehicle. 

\subsection{Greedy}
\label{sec:greedy}
Removing orders in the most costful positions and reinserting it in its cheapest (greedy) position seems to be a reasonable way of moving towards a better solution.
We therefore propose a heuristic that removes the orders with the highest cost $c_{i}$.
The $c_i$ is calclated as the increase in a vehicles schedule cost with the chosen order.

\begin{algorithm}
    \caption{Greedy heuristic}\label{alg:greedy}
    \begin{algorithmic}[1]
        \Function{greedy}{$s\in \{solution}$, $p$}
        \State select the number of orders to reinsert $q$
        \State add orders in array $A$ based on decending cost $f(S_v) - f_{-i}(S_v)$
        \State empty order set $I$
        \Repeat
            \State choose a random number $y$ between $[0,1]$
            \State remove the order in position $y^p$ in $A$ from $s$ 
            \State add removed order to $I$
        \Until {$|I| = q$}
        \Repeat
            \State sort $I$ based on each orders minimum increase in objective value $c_i$
            \State insert the first order $i$ from $I$ in its best possible position in $s$
            \State remove order $i$ from $I$
        \Until{$|I| = 0$}
        \State
        \Return $s$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

We remove orders again based on the same randomness factor explained above. 
We do this by first sorting the orders decending based on $c_i$. $c_i$ can be calculated as $c_i=f(S_v) - f_{-i}(S_v)$. 
Here the $-i$ indicate that the vehicle schedule $S_v$ is without the order $i$.
We then choose the order in the $y^p$ position. \par

To reinsert an order we simply sort the removed orders based on their $c_i$.
The $c_i = \min(\Delta f_{iv})$, were the $\Delta f_{iv}$ is the lowest increase in objective function value when inserting $i$ in a vehicle schedule $v$. The $c_i$ is therefore the minimum of theseincreases. 
We reinsert the order in its best possible position in vehicle schedule $v$ and update the solution $s$. 

\subsection{Similar regret}
\label{sec:shaw}
The removal part of this heuristic is based on \citet{shaw97}'s removal heuristic with slight modifications based on our problem. 
It removes orders that share specific similar qualities. 
The basic idea is that replacing these orders by eachother will find new, hopefully better, solutions. 
We define a relatedness factor $r_{ij}$ which represents how much the order $i$ is related to the order $j$. 
The lower the value of $r_{ij}$ the more the two orders $i$ and $j$ are related.
The relatedness of two orders were based on the following properties: 
a distance property, a weight property, an overlapping timewindow property, a property indicating if the same vehicles can be used to serve each request, and finally if the orders belong to the same factory.

The relatedness factor is given by the following equation:

\begin{equation}
\label{relatedness}
    r_{ij} = \psi ( D_{i j} + D_{(i+n)(j+n)}) + \omega|Q_i - Q_j|
    + \phi (1-\dfrac{|V_i\cap V_j|}{max(|V_i|, |V_j|)} ) + \tau G_{ij} + \chi (U_{ij} + U_{(i+n)(j+n)})
\end{equation}
Thus the relatedness measure $r_{ij}$ is given a value $0\leq r_{ij} \leq 2\psi + \omega + \phi + \tau  + \chi$. 
We have chosen the following values in this paper $\psi=0.7$, $\omega=1.0$, $\phi=0.8$, $\tau=0.3$, $\chi = 0.3$. \par

$D_{ij}$, $Q_i$, are the same as in \cref{ch:mm} and all values have been normalised to result in a value between $[0..1]$. 
$V_i$ is the set of vehicles that can serve order $i$. 
The parameter $G_{ij}$ is 1 if $i$ belong to another factory than $j$ and 0 if they belong to the same factory. 
$U_{ij}$ is the timewindows at the pickup and delivery location, equal to 0 if the two orders have identical time windows and 1 if not. It corresponds to the sum of overlapping time windows divided by the overlapping span of the two time window sets.
It can be formulated as follows:
\begin{equation}
    \label{overlaptime}
    U_{ij} = 1 - 
    \dfrac{T^O_{ij}}
    {\max{(\max\limits_{p\in \pi_i} \overline{T_{ip}}, \max\limits_{o\in \pi_j} \overline{T_{jo}})} - 
    \min{(\min\limits_{p\in \pi_i} \underline{T_{ip}}, \min\limits_{o\in \pi_j} \underline{T_{jo}}}) -  T^{NO}_{ij} }
\end{equation}

Here $\overline{T_{ip}}$ and $\underline{T_{ip}}$ are the upper and lower time windows defined in the problem formulation.
The factor $T^O_{ij}$ consists of all the timewindows where order $i$ overlaps with the timewindows from order $j$. It can be written mathematically as follows:
\begin{equation}\label{eq:ot}
    T^O_{ij}=\sum\limits_{\substack{p\in \pi_i\\ o\in \pi_j\\ \underline{T_{ip}}\leq \overline{T_{jo}}\\ \underline{T_{jo}}\leq\overline{T_{ip}}}} (\min(\overline{T_{ip}}, \overline{T_{jo}}) - \max(\underline{T_{ip}},\underline{T_{jo}}) )
\end{equation}

The factor $T^{NO}_{ij}$ is the opposite of the above factor $T^O_{ij}$ and represent the time when neither $i$ not $j$ have a timewindow. This can be examplified by night time when no factory is open. It can be formulated as follow:

\begin{equation}\label{eq:not}
    T^{NO}_{ij} = \sum\limits_{\substack{p\in \pi_i\\ o\in \pi:q_j\\ \underline{T_{ip}}\geq \overline{T_{j(o-1)}}\\ \underline{T_{jo}}\geq\overline{T_{i(p-1)}}}} 
    (\min(\underline{T_{ip}}, \underline{T_{jo}}) - \max(\overline{T_{i(p-1)}},\overline{T_{j(o-1)}}) ) 
\end{equation}

\begin{algorithm}
    \caption{Similar regret heuristic}\label{alg:simReg}
    \begin{algorithmic}[1]
        \Function{similarRegret}{$s\in \{solution}$, $p$}
        \State select the number of orders to reinsert $q$
        \State select a random order $i$ from $s$
        \State add $i$ to order set $I$
        \Repeat
            \State add all orders $j\not\in I$ in array $A$ acending based on relatedness $r_{ij}$
            \State choose a random number $y$ between $[0,1]$
            \State remove the order in position $y^p$ in $A$ from $s$ 
            \State add removed order to $I$
        \Until {$|I| = q$}
        \Repeat
            \State sort $I$ acending based on regret value $c^*_i$
            \State insert the first order $z$ from $I$ in its best possible position in $S_v$
            \State update $s$ based on $S_v$
            \State remove order $z$ from $I$
        \Until{$|I| = 0$}
        \State
        \Return $s$
        \EndFunction
    \end{algorithmic}
\end{algorithm}


\Cref{alg:simReg} is s pseudocode of the complete similar regret heursitic.
It starts by removing a random order $i$ from the solution and adding it to a set $I$. 
It then creates an array and adds all orders $j\not\in I$ to this array, before it sorts it acending based on $r_{ij}$.
It then selects the order with the $y^p$ highest relatedness to remove from $s$ and add to $I$. This continues until $q$ orders have been removed.
The insertion part of this heuristic tries to improve on insertion algorithm from \cref{sec:greedy} by calculating a regret value, $c^*_i$. The regret value tries to predict the "what if i insert later" value of an order $i$. 
If we let $v_{ik} \in \{ 1,..,m\}$ represent a vehicle schedule for which $i$ has the $k$ lowest insertion cost. 
That means $\Delta f_{s_{ik}} \leq \Delta f_{s_{ik'}}$ for $k\leq k'$.
We can then define the regret value as follows:
\begin{equation}\label{eq:regret}
    \centering
    c^*_i=\Delta f_{v_{i2}} - \Delta f_{v_{i1}} + \Delta f_{v_{i3}} - \Delta f{v_{i2}}
\end{equation}

The $c_i$ therefore represent the difference in inserting order $i$ in its best position and its second best position plus the difference in inserting it in its second best position and its third best position. In each iteration the heuristic chooses to insert the order with the highest $c^*_i$. The order will be inserted in its best possible position. Ties were broken by choosing the order with the lowest $c_i$.

\section{Choosing a heuristic}
\label{sec:choosing}
We proposed several heuristics of different classes in the previous sections, and one could simply choose one of them and use them throughout the search. 
However we propose to use all the heuristics, for now. 
The reason for doing so is that the swap heuristic from \cref{sec:swap} could be good for one type of instance, while the similar regret heuristic from \cref{sec:exch} might be good for another type of instance. 
We think that alternating between several type of heuristics gives us amore robust algorithm. 
We will however do alot of testing in \cref{ch:res}, which will lead us to a more compact version of the model. \par
To select a heuristic to use in each iteration of \cref{alg:alns}, we have used a what can be refered to as a \emph{roulette weel principle}. 
This means that we represent each heuristic by a number $i \in [1..m]$ where m is the amount of heuristics. 
We select a heuristic with a probability $p_i$ calculated based on each heuristics \textit{weight} $w_i$ as follows:
\begin{equation}\label{eq:weights}
    p_i=\dfrac{w_i}{\sum_{j=1}^m w_j}
\end{equation}

These weights could be set fixed per problem but we choose to use an adaptive weight system explained futher in \cref{sec:weight}.

\section{Adaptive weight adjustment}
\label{sec:weight}
The weights from \cref{sec:choosing} can be adapted automatically by the algorithm. \cite{ropke06} implemented an adaptive weight system in a Large neighbourhood search model.
The basic idea is to keep track of the perfomance of each heuristic through a scoring system. 
A heuristic is given a higher score for a better performance and a low score for low performance. 
The entire \cref{alg:alns} is divided into \textit{segments}, or a numer of iterations. 
We have used segments here of 100 iterations.
At the beginning of the algorithm, each heuristic is given the same weights, resulting in equal probability in selecting each heuristic for the first segment.
Throughout a segment, each heuristic is rewarded points based on the following system:
\begin{itemize}
    \item Finding a new global best solution is given a high score to the heuristic for that iteration.
    \item Finding a new solution that is better than the current solution gives a medium score to the heuristic for that iteration.
    \item Finding a new solution that has not been found before is rewarded a small score to the heuristic for that iteration.
\end{itemize}
After a segment, the sum of the scores for each heuristic are used to update the weights. If we let $w_{ij}$ be the weight of heuristic $i$ in segment $j$ the update would be as follows:
\begin{equation}\label{eq:update}
    w_{ij} = w_{i(j-1)}h + (1-h)\dfrac{\pi_i}{\lambda_i}
\end{equation}

The $h$ represent here a historical weight factor which we have set to $80\%$ meaning we let the previous weight compose $80\%$ of the new weight. 
$20\%$ of the current weight $w_{ij}$ is composed of the score $\pi_i$ divided over the times we have run the heuristic $\lambda_i$ during the current segment.

\begin{figure}                                                                    
    \centering                                                                                     
    \includegraphics[width=.7\textwidth]{weight_probability}                                     
    \caption{The figure illustrates an example of the developement of the weight probability $p_i$ when running our model. The x-axis represent a segment and the y-axis the probability of selecting a heuristic.}
    \label{fig:weights}
\end{figure}

\Cref{fig:weights} show how the weight probability $p_i$ developes for five different heuristics in an example from our model. 
We observe that the probability start out equal but that some heuristics are getting more probability after only a few segments.
The figure also shows that we have put a lower limit to the probility to make certain that every heuristic will be selected at least a few times during a segment.

\section{Acceptance criteria and stopping condition}
\label{sec:accept}
We could only accept solutions that are better than the current one.
However this could lead our model to get problems with diversification and not be able to get explore the entire solution space. 
We have therefore chosen to use the acceptance criteria used by simmulated annealing, mentioned in \cref{ch:methods}. 
This acceptance criteria is made so that it accepts a solution that is better than the current solution. It also accepts a worse solution with the probability $e^{-|f-f_{new}|/T}$, where $T<0$ is the temperature. 
The cooling schedule we use here was implemented by \cite{crama03}. That sets a certain starting temperature $T_{start}$ that decreases per iteration with a certain cooling rate $0<c<1$.
We wanted to let $T_{start}$ depend on the problem instance our model is trying to solve. Therefore we run 100 iterations with a fixed acceptance rate of $a = 0.8$. 
We calculate the average of all worse solutions that are accepted over these iterations $f^T_{average}$.  
Then we use this to calculate the fitting starting temperature as follows:
\begin{equation}\label{eq:startTemp}
    T_{start} = \dfrac{f^T_{average}}{log(a)}
\end{equation}

The algorithm stops when a specified number of iterations are reached which we specify here as 10 thousand iterations per run.

\section{Wild escape algorithm}
\label{sec:wild}
Algorithms containing large neighbourhood search heuristics, such as our model, are known to be good at searching locally aswell as globally. 
However considering that our model is more focused on instensification, it could be that we end up getting stuck in a part of the solution space. It is important that our algorithm is able to react in these situations.
In \cref{alg:alns} we see in \cref{line:escpCond} that if an escape condition is fulfilled, we will run a Wild Escape function. 
We set the criteria that if we, for 500 iterations, do not find an improvement in $s_{best}$, we will perform this action.

\begin{algorithm}
    \label{alg:wild}
    \caption{Wild escape algorithm}
    \begin{algorithmic}[1]
        \Function{wildEscape}{solution $s$, $s_{best}$, set of heuristics $W$}
        \Repeat
            \State choose a random heuristic $h$ from $W$
            \State apply $h$ to $s$
            \If {$f(s)<f(s_{best})$} \label{line:best}
                \State $s_{best} = s$ \label{line:best2}
            \EndIf
        \Until stop condition met \label{line:stopCond}
        \State \Return $s$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

The algorithms pseudocode is described in \cref{alg:wild}. 
The stopping condition, in \cref{line:stopCond}, is 20 iterations. 
The algorithm accepts any new solution found regardless of the objective value to move as far away from the current solution as possible.
\Crefrange{line:best}{line:best2} in \cref{alg:wild} are saying that if we happen to find a better solution $s_{best}$ on our way out of the current solution space, we should remember it. We still continue moving away from the current solution $s$.
The heuristics used by the escape algorithm are the heuristics random fit from \cref{sec:rand}, 3-exchange from \cref{sec:exch} and swap \cref{sec:swap}. 
The heuristics have an increased size of $q$ to increase diversification. 
We chose these heuristics because they are not trying to improve the solution in any specific way and select targeted solutions, but rather moves randomly around the solution space. 
\biblio                                                         
\end{document}
